{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca9cf8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "Title & Name\n",
    "Title: Multimodal Cooking Assistant using Image and Ingredient Inputs\n",
    "Name: Akshat Aren\n",
    "210104012\n",
    "\n",
    "■ Motivation\n",
    "The idea of building a cooking assistant came from the practical need for intelligent recipe systems that can interpret food images and \n",
    "ingredients to suggest meaningful cooking instructions. With the rise of smart kitchens and diet tracking apps, creating a model that\n",
    " understands multiple input types (text and image) is both exciting and useful.\n",
    "\n",
    "■ Multimodal Learning: Historical Context\n",
    "Multimodal learning has rapidly evolved in recent years with advances in computer vision and natural language processing.\n",
    " Earlier models processed single modalities (text-only or image-only), but modern systems like CLIP (2021) and ALIGN \n",
    " demonstrate how fusing visual and textual data leads to richer understanding. This project builds on that trend by \n",
    " ombining food images with ingredient lists to produce structured cooking instructions—a real-world application of multimodal AI.\n",
    "\n",
    "■ Key Learnings from This Work\n",
    "Learned how to encode and fuse different data modalities (image + text).\n",
    "\n",
    "Understood the challenge of aligning visual content with sequential text (instructions).\n",
    "\n",
    "Gained experience working with pretrained models (CNNs, LSTMs), cosine similarity loss, and embedding visualization.\n",
    "\n",
    "Developed skills in building interpretable architectures for complex generative tasks.\n",
    "\n",
    "■ Code / Notebook (Demos)\n",
    "Implemented in Python using PyTorch and Jupyter.\n",
    "\n",
    "Visual demos include:\n",
    "\n",
    "Instruction generation from food images and ingredients.\n",
    "\n",
    "Embedding space analysis (t-SNE plots, nearest neighbor search).\n",
    "\n",
    "Failure cases visualized to diagnose misalignment.\n",
    "\n",
    "Notebook link or screenshots can be added here.\n",
    "\n",
    "■ Reflections\n",
    "(a) What surprised you?\n",
    "\n",
    "The model sometimes generated impressively accurate instructions even with minimal data. Also, embedding \n",
    "rithmetic (like \"chicken + salad − pizza ≈ chicken salad\") worked better than expected.\n",
    "\n",
    "(b) Scope for Improvement:\n",
    "\n",
    "Add support for quantities and cooking times.\n",
    "\n",
    "Improve instruction coherence with transformer-based decoders.\n",
    "\n",
    "Include dietary filters and user preferences for personalized recipe generation.\n",
    "\n",
    "Extend to multilingual recipe generation and real-time voice interaction.\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
